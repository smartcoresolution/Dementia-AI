{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davechang-99/Dementia-AI/blob/main/dementia_pre_detaction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnnH260PQvbA"
      },
      "source": [
        "ì…€ 1: í™˜ê²½ ì„¤ì • ë° ëŸ°íƒ€ì„ ì¬ì‹œì‘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DurFXoppPTcp",
        "outputId": "5475e070-ead0-49cc-8cbc-f71be8fb16f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.40.1\n",
            "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/138.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (2025.7.14)\n",
            "Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.0\n",
            "    Uninstalling transformers-4.54.0:\n",
            "      Successfully uninstalled transformers-4.54.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.1\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 1.9.0 requires torch>=2.0.0, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "sentence-transformers 4.1.0 requires torch>=1.11.0, which is not installed.\n",
            "peft 0.16.0 requires torch>=1.13.0, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m819.2/819.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ViT ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ transformers ë²„ì „ ê³ ì • ë° torch, numpy, librosa ë“± ì£¼ìš” íŒ¨í‚¤ì§€ì˜ í˜¸í™˜ ë²„ì „ ì¬ì„¤ì¹˜ í›„ ëŸ°íƒ€ì„ ì¬ì‹œì‘\n",
        "!pip install transformers==4.40.1\n",
        "!pip uninstall -y numpy matplotlib torch torchvision torchaudio librosa > /dev/null\n",
        "!pip install numpy==1.26.4 matplotlib==3.8.0 librosa==0.10.1 --quiet\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n",
        "# ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸\n",
        "import numpy, torch, torchvision, librosa\n",
        "print(\"âœ… numpy:\", numpy.__version__)\n",
        "print(\"âœ… torch:\", torch.__version__)\n",
        "print(\"âœ… torchvision:\", torchvision.__version__)\n",
        "print(\"âœ… librosa:\", librosa.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KrK-baMQ6gx"
      },
      "source": [
        "ì…€ 2: GPU/CPU ì¥ì¹˜ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU8OOkBsPZt9",
        "outputId": "703aa536-ec70-4010-ed5a-2a6103a81246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEVICE] Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# PyTorchë¥¼ ì´ìš©í•˜ì—¬ CUDAê°€ ê°€ëŠ¥í•œ ê²½ìš° GPU, ì•„ë‹ˆë©´ CPU ì¥ì¹˜ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ì—°ì‚° í™˜ê²½ ì„¤ì •\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[DEVICE] Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aN2UucGSWY3"
      },
      "source": [
        "ì…€ 3: Google Drive ë§ˆìš´íŠ¸ ë° ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw8vg6a8PZw2",
        "outputId": "27a09718-3b82-4f75-efd4-c9e04a1a65bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "ğŸ“‚ ë°ì´í„°ì…‹ ìœ„ì¹˜: /content/drive/MyDrive/DATASET_A\n"
          ]
        }
      ],
      "source": [
        "# âœ… Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# âœ… ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
        "dataset_path = Path(\"/content/drive/MyDrive/DATASET_A\")\n",
        "print(\"ğŸ“‚ ë°ì´í„°ì…‹ ìœ„ì¹˜:\", dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeJsVmL2RAC4"
      },
      "source": [
        "ì…€ 4: í´ë” êµ¬ì¡° ì¶œë ¥ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1iB_r8fPZzs",
        "outputId": "770ef8df-d652-49a9-b364-9269cbc0d136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í´ë” êµ¬ì¡°ë§Œ ì¶œë ¥:\n",
            "ğŸ“ DATASET_A/\n",
            "    ğŸ“ Dementia/\n",
            "        ğŸ“ train/\n",
            "        ğŸ“ val/\n",
            "    ğŸ“ Normal/\n",
            "        ğŸ“ train/\n",
            "        ğŸ“ val/\n"
          ]
        }
      ],
      "source": [
        "# âœ… í•˜ìœ„ í´ë” êµ¬ì¡°ë§Œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
        "def print_folder_tree(path: Path, indent: str = \"\"):\n",
        "    if path.is_dir():\n",
        "        print(f\"{indent}ğŸ“ {path.name}/\")\n",
        "        for child in sorted(path.iterdir()):\n",
        "            if child.is_dir():\n",
        "                print_folder_tree(child, indent + \"    \")\n",
        "\n",
        "# âœ… í´ë” êµ¬ì¡° ì¶œë ¥\n",
        "print(\"âœ… í´ë” êµ¬ì¡°ë§Œ ì¶œë ¥:\")\n",
        "print_folder_tree(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgtnavQ_RDyI"
      },
      "source": [
        "ì…€ 5: Mel-spectrogram ì´ë¯¸ì§€ ë³€í™˜ ë° ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsO3Knn1PdTM",
        "outputId": "4d4ccc37-ced7-413d-da19-19b5f2f6ec13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§ ë³€í™˜ ì¤‘: Normal/train â†’ ì´ 200ê°œ íŒŒì¼\n",
            "ğŸ§ ë³€í™˜ ì¤‘: Normal/val â†’ ì´ 99ê°œ íŒŒì¼\n",
            "ğŸ§ ë³€í™˜ ì¤‘: Dementia/train â†’ ì´ 240ê°œ íŒŒì¼\n",
            "ğŸ§ ë³€í™˜ ì¤‘: Dementia/val â†’ ì´ 60ê°œ íŒŒì¼\n",
            "âœ… ëª¨ë“  Mel-spectrogram ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import os\n",
        "\n",
        "# âœ… ì›ë³¸ ìŒì„± ë°ì´í„° ê²½ë¡œ\n",
        "dataset_path = Path(\"/content/drive/MyDrive/DATASET_A\")\n",
        "\n",
        "# âœ… Mel-spectrogram ì €ì¥ ê²½ë¡œ\n",
        "mel_output_dir = Path(\"/content/MEL_DATASET\")\n",
        "\n",
        "# âœ… í´ë˜ìŠ¤ ë° ë¶„í•  ë¦¬ìŠ¤íŠ¸\n",
        "classes = [\"Normal\", \"Dementia\"]\n",
        "splits = [\"train\", \"val\"]\n",
        "\n",
        "# âœ… ë™ì¼í•œ í´ë” êµ¬ì¡° ìƒì„±\n",
        "for cls in classes:\n",
        "    for split in splits:\n",
        "        target_dir = mel_output_dir / cls / split\n",
        "        target_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# âœ… Mel-spectrogram ì €ì¥ í•¨ìˆ˜ ì •ì˜\n",
        "def save_mel(wav_path, save_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(wav_path, sr=16000)\n",
        "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "        plt.figure(figsize=(2.24, 2.24), dpi=100)\n",
        "        librosa.display.specshow(mel_db, sr=sr, cmap='viridis')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì—ëŸ¬ ë°œìƒ: {wav_path} â†’ {e}\")\n",
        "\n",
        "# âœ… ë³€í™˜ ì‹¤í–‰\n",
        "for cls in classes:\n",
        "    for split in splits:\n",
        "        input_dir = dataset_path / cls / split\n",
        "        output_dir = mel_output_dir / cls / split\n",
        "\n",
        "        wav_files = sorted(input_dir.glob(\"*.wav\"))\n",
        "        print(f\"ğŸ§ ë³€í™˜ ì¤‘: {cls}/{split} â†’ ì´ {len(wav_files)}ê°œ íŒŒì¼\")\n",
        "\n",
        "        for wav_path in wav_files:\n",
        "            output_path = output_dir / (wav_path.stem + \".png\")\n",
        "            save_mel(wav_path, output_path)\n",
        "\n",
        "print(\"âœ… ëª¨ë“  Mel-spectrogram ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIlLv7TsRGzu"
      },
      "source": [
        "ì…€ 6: Dataset í´ë˜ìŠ¤, ì „ì²˜ë¦¬ ë° DataLoader ì •ì˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVE__AtBYl6O"
      },
      "outputs": [],
      "source": [
        "# âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# âœ… ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ë³€í™˜ í´ë˜ìŠ¤ ì •ì˜\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "# âœ… Mel-spectrogram ì´ë¯¸ì§€ Dataset í´ë˜ìŠ¤\n",
        "class MelImageDataset(Dataset):\n",
        "    def __init__(self, base_dir, split=\"train\", transform=None):\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "        self.classes = [\"Normal\", \"Dementia\"]\n",
        "\n",
        "        for label, cls in enumerate(self.classes):\n",
        "            cls_path = Path(base_dir) / cls / split\n",
        "            img_paths = sorted(cls_path.glob(\"*.png\"))\n",
        "            self.samples.extend(img_paths)\n",
        "            self.labels.extend([label] * len(img_paths))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.samples[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "# âœ… í›ˆë ¨ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ë°ì´í„° ì¦ê°• í¬í•¨)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ToTensor(),\n",
        "    AddGaussianNoise(mean=0., std=0.01)\n",
        "])\n",
        "\n",
        "# âœ… ê²€ì¦ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ë°ì´í„° ì¦ê°• ì—†ìŒ)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# âœ… Dataset ë° DataLoader ì •ì˜\n",
        "base_path = \"/content/MEL_DATASET\"\n",
        "train_data = MelImageDataset(base_path, split=\"train\", transform=train_transform)\n",
        "val_data = MelImageDataset(base_path, split=\"val\", transform=val_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Zu3vmsRLoZ"
      },
      "source": [
        "ì…€ 7: CNN ë° ViT ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY5TvLDXPdY0"
      },
      "outputs": [],
      "source": [
        "# âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "\n",
        "# CNN ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ViT ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜\n",
        "class ViTClassifier(nn.Module):\n",
        "    # processorë¥¼ __init__ì—ì„œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ì—¬ íš¨ìœ¨ì„± ê°œì„ \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "        self.fc = nn.Linear(self.vit.config.hidden_size, 2)\n",
        "        self.processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "    # forward í•¨ìˆ˜ ë‚´ì—ì„œ í”„ë¡œì„¸ì„œë¥¼ ì¬ì‚¬ìš©\n",
        "    def forward(self, x):\n",
        "        # í…ì„œ ì´ë¯¸ì§€ë¥¼ PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "        pil_images = [transforms.ToPILImage()(img.cpu()) for img in x]\n",
        "        # í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë°ì´í„° ì¤€ë¹„\n",
        "        inputs = self.processor(images=pil_images, return_tensors=\"pt\").to(x.device)\n",
        "        outputs = self.vit(**inputs)\n",
        "        return self.fc(outputs.last_hidden_state[:, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P5DpZjlRPYl"
      },
      "source": [
        "ì…€ 8: Adaboostìš© ë°ì´í„° ë¡œë”© í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TStko4Mvai8T",
        "outputId": "d8051a69-dce9-4f4d-beb9-10ebad36925d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í›ˆë ¨ ë°ì´í„° í¬ê¸°: (440, 4096), ë ˆì´ë¸”: (440,)\n",
            "ê²€ì¦ ë°ì´í„° í¬ê¸°: (159, 4096), ë ˆì´ë¸”: (159,)\n",
            "\n",
            "âœ… í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê°„ì— ì¤‘ë³µ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Adaboost ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¬í™•ì¸í•˜ì„¸ìš”.\n"
          ]
        }
      ],
      "source": [
        "# âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import os # os ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€\n",
        "\n",
        "# Adaboostìš© ì´ë¯¸ì§€ ë¡œë”© í•¨ìˆ˜ (í´ë” êµ¬ì¡° ë°˜ì˜)\n",
        "def load_images_and_labels(base_dir, split=\"train\", size=(64, 64)):\n",
        "    X, y = [], []\n",
        "    classes = [\"Normal\", \"Dementia\"]\n",
        "\n",
        "    for label, cls in enumerate(classes):\n",
        "        cls_path = Path(base_dir) / cls / split\n",
        "        img_paths = sorted(cls_path.glob(\"*.png\"))\n",
        "\n",
        "        for path in img_paths:\n",
        "            img = Image.open(path).convert(\"L\").resize(size)\n",
        "            X.append(np.array(img).flatten())\n",
        "            y.append(label)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# âœ… base_dirëŠ” MEL_DATASET í´ë”ì˜ ë£¨íŠ¸ ê²½ë¡œ\n",
        "base_dir = \"/content/MEL_DATASET\"\n",
        "\n",
        "# í›ˆë ¨ìš© ì´ë¯¸ì§€ ë²¡í„° ë¡œë”©\n",
        "X_train, y_train = load_images_and_labels(base_dir, split=\"train\")\n",
        "\n",
        "# ê²€ì¦ìš© ì´ë¯¸ì§€ ë²¡í„° ë¡œë”©\n",
        "X_val, y_val = load_images_and_labels(base_dir, split=\"val\")\n",
        "\n",
        "print(f\"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X_train.shape}, ë ˆì´ë¸”: {y_train.shape}\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„° í¬ê¸°: {X_val.shape}, ë ˆì´ë¸”: {y_val.shape}\")\n",
        "\n",
        "# âœ… ë°ì´í„°ì…‹ ì¤‘ë³µ í™•ì¸ ì½”ë“œ ì¶”ê°€\n",
        "train_filenames = {os.path.basename(p) for p in (Path(base_dir) / \"Normal\" / \"train\").glob(\"*.png\")}\n",
        "train_filenames.update({os.path.basename(p) for p in (Path(base_dir) / \"Dementia\" / \"train\").glob(\"*.png\")})\n",
        "\n",
        "val_filenames = {os.path.basename(p) for p in (Path(base_dir) / \"Normal\" / \"val\").glob(\"*.png\")}\n",
        "val_filenames.update({os.path.basename(p) for p in (Path(base_dir) / \"Dementia\" / \"val\").glob(\"*.png\")})\n",
        "\n",
        "# êµì§‘í•©ì„ ì°¾ì•„ ì¤‘ë³µ íŒŒì¼ í™•ì¸\n",
        "overlap = train_filenames.intersection(val_filenames)\n",
        "\n",
        "if len(overlap) > 0:\n",
        "    print(f\"\\nâŒ ê²½ê³ : í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ì— ì¤‘ë³µ íŒŒì¼ì´ {len(overlap)}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"ì¤‘ë³µ íŒŒì¼ ì˜ˆì‹œ: {list(overlap)[:5]}\")\n",
        "    print(\"â†’ Adaboost ëª¨ë¸ì˜ ì™„ë²½í•œ ì„±ëŠ¥ì€ ì´ ì¤‘ë³µ ë•Œë¬¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(\"\\nâœ… í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê°„ì— ì¤‘ë³µ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Adaboost ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¬í™•ì¸í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7rqcfbGxYVS"
      },
      "source": [
        "ì…€ 9: ëª¨ë¸ í•™ìŠµ(Random Forest) ë° ìµœì¢… í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NLg2mQ5is4o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier # RandomForestClassifier ì¶”ê°€\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# âœ… í•™ìŠµ í•¨ìˆ˜\n",
        "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            preds = model(images)\n",
        "            loss = criterion(preds, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•¨ìˆ˜\n",
        "def tune_cnn_hyperparameters(train_data, val_data):\n",
        "    learning_rates = [1e-3, 1e-4]\n",
        "    batch_sizes = [16, 32]\n",
        "    epochs = 5\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_params = {}\n",
        "\n",
        "    print(\"--- CNN í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ---\")\n",
        "    for lr in learning_rates:\n",
        "        for bs in batch_sizes:\n",
        "            print(f\"\\n[íŠœë‹] LR: {lr}, ë°°ì¹˜ í¬ê¸°: {bs}\")\n",
        "            train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
        "            val_loader = DataLoader(val_data, batch_size=bs)\n",
        "\n",
        "            cnn_model = CNNClassifier()\n",
        "            accuracy = train_model(cnn_model, train_loader, val_loader, epochs=epochs, lr=lr)\n",
        "\n",
        "            print(f\"âœ… ê²€ì¦ ì •í™•ë„: {accuracy:.2f}%\")\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = {'lr': lr, 'batch_size': bs}\n",
        "\n",
        "    print(\"\\n--- íŠœë‹ ê²°ê³¼ ---\")\n",
        "    print(f\"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}\")\n",
        "    print(f\"âœ… ìµœê³  ê²€ì¦ ì •í™•ë„: {best_accuracy:.2f}%\")\n",
        "    return best_params\n",
        "\n",
        "# -----------------\n",
        "# 1. ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ CNN ëª¨ë¸ ì¬í•™ìŠµ ë° í‰ê°€\n",
        "# -----------------\n",
        "best_cnn_params = tune_cnn_hyperparameters(train_data, val_data)\n",
        "epochs = 10\n",
        "\n",
        "print(\"\\n--- ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ CNN ëª¨ë¸ í•™ìŠµ ---\")\n",
        "train_loader_cnn = DataLoader(train_data, batch_size=best_cnn_params['batch_size'], shuffle=True)\n",
        "val_loader_cnn = DataLoader(val_data, batch_size=best_cnn_params['batch_size'])\n",
        "cnn_model = CNNClassifier()\n",
        "train_model(cnn_model, train_loader_cnn, val_loader_cnn, epochs=epochs, lr=best_cnn_params['lr'])\n",
        "\n",
        "# CNN ëª¨ë¸ ì˜ˆì¸¡ (í™•ë¥  í¬í•¨)\n",
        "cnn_model.eval()\n",
        "cnn_preds_proba = []\n",
        "cnn_preds = []\n",
        "cnn_true = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader_cnn:\n",
        "        images = images.to(device)\n",
        "        outputs = cnn_model(images)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        cnn_preds_proba.extend(probabilities.cpu().numpy()[:, 1])\n",
        "        cnn_preds.extend(predicted.cpu().numpy())\n",
        "        cnn_true.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# 2. ViT ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
        "# -----------------\n",
        "print(\"\\n--- ViT ëª¨ë¸ í•™ìŠµ ---\")\n",
        "train_loader_vit = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader_vit = DataLoader(val_data, batch_size=16)\n",
        "vit_model = ViTClassifier()\n",
        "train_model(vit_model, train_loader_vit, val_loader_vit, epochs=5)\n",
        "\n",
        "# ViT ëª¨ë¸ ì˜ˆì¸¡ (í™•ë¥  í¬í•¨)\n",
        "vit_model.eval()\n",
        "vit_preds_proba = []\n",
        "vit_preds = []\n",
        "vit_true = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader_vit:\n",
        "        images = images.to(device)\n",
        "        outputs = vit_model(images)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        vit_preds_proba.extend(probabilities.cpu().numpy()[:, 1])\n",
        "        vit_preds.extend(predicted.cpu().numpy())\n",
        "        vit_true.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# 3. Adaboostë¥¼ ëŒ€ì²´í•  RandomForest ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
        "# -----------------\n",
        "print(\"\\n--- RandomForest ëª¨ë¸ í•™ìŠµ ---\")\n",
        "# n_estimators: íŠ¸ë¦¬ì˜ ê°œìˆ˜, max_depth: íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# RandomForest ëª¨ë¸ í›ˆë ¨ ë°ì´í„° ì„±ëŠ¥ í™•ì¸\n",
        "y_pred_rf_train = rf_model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_pred_rf_train) * 100\n",
        "print(f\"\\n--- RandomForest ëª¨ë¸ í›ˆë ¨ ë°ì´í„° ì •í™•ë„: {train_accuracy:.2f}% ---\")\n",
        "\n",
        "# RandomForest ëª¨ë¸ ì˜ˆì¸¡ (í™•ë¥  í¬í•¨)\n",
        "y_pred_rf_proba = rf_model.predict_proba(X_val)[:, 1]\n",
        "y_pred_rf = rf_model.predict(X_val)\n",
        "y_true_rf = y_val\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# 4. ìµœì¢… ê²°ê³¼ ì‹œê°í™” ë° ì„±ëŠ¥ ë¹„êµ í‘œ\n",
        "# -----------------\n",
        "def plot_results(y_true, y_pred, y_proba, model_name):\n",
        "    print(f\"\\n--- {model_name} ê²°ê³¼ ---\")\n",
        "    report = classification_report(y_true, y_pred, target_names=[\"Normal\", \"Dementia\"], output_dict=True)\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Dementia\"]))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"Dementia\"], yticklabels=[\"Normal\", \"Dementia\"])\n",
        "    plt.title(f\"{model_name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f\"âœ… {model_name} AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{model_name} Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return report, roc_auc\n",
        "\n",
        "# ê° ëª¨ë¸ë³„ ê²°ê³¼ ì‹œê°í™” ë° ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘\n",
        "cnn_report, cnn_auc = plot_results(cnn_true, cnn_preds, cnn_preds_proba, \"CNN\")\n",
        "vit_report, vit_auc = plot_results(vit_true, vit_preds, vit_preds_proba, \"ViT\")\n",
        "rf_report, rf_auc = plot_results(y_true_rf, y_pred_rf, y_pred_rf_proba, \"RandomForest\")\n",
        "\n",
        "\n",
        "# âœ… ì„±ëŠ¥ ë¹„êµ í‘œ ìƒì„± ë° ì¶œë ¥\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['CNN', 'ViT', 'RandomForest'],\n",
        "    'Accuracy': [cnn_report['accuracy'], vit_report['accuracy'], rf_report['accuracy']],\n",
        "    'Precision': [cnn_report['weighted avg']['precision'], vit_report['weighted avg']['precision'], rf_report['weighted avg']['precision']],\n",
        "    'Recall': [cnn_report['weighted avg']['recall'], vit_report['weighted avg']['recall'], rf_report['weighted avg']['recall']],\n",
        "    'F1-Score': [cnn_report['weighted avg']['f1-score'], vit_report['weighted avg']['f1-score'], rf_report['weighted avg']['f1-score']],\n",
        "    'AUC': [cnn_auc, vit_auc, rf_auc]\n",
        "}).set_index('Model')\n",
        "\n",
        "print(\"\\n--- ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ ---\")\n",
        "print(results_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}